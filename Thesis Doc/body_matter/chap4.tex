% ΟΙ ΑΡΧΙΤΕΚΤΟΝΙΚΕΣ ΠΟΥ ΧΡΗΣΙΜΟΠΟΙΩ ΣΤΑ ΠΕΙΡΑΜΑΤΑ ΚΑΙ Η ΛΕΙΤΟΥΡΓΙΑ ΤΟΥΣ 
\chapter{Αρχιτεκτονικές Τμηματοποίησης που Χρησιμοποιήθηκαν}
\label{ch:architectures}

\InitialCharacter{Σ}το κεφάλαιο αυτό παρουσιάζονται οι αρχιτεκτονικές που χρησιμοποιήθηκαν στα πειράματα της παρούσας
εργασίας. Όλα τα μοντέλα αφορούν \textbf{τμηματοποίηση \en{3D} ιατρικών δεδομένων} και υλοποιούνται μέσω της βιβλιοθήκης
\en{MONAI} \cite{cardoso2022monai}, η οποία παρέχει έτοιμες υλοποιήσεις και καθιερωμένα \en{building blocks}.

Η οικογένεια μοντέλων που εξετάζεται μπορεί να ομαδοποιηθεί ως εξής:
\begin{itemize}
  \item \textbf{Καθαρά συνελικτικά (\en{CNN-based})}: \en{UNet}, \en{DynUNet}, \en{SegResNet}, \en{AttentionUnet}.
  \item \textbf{\en{Transformer}-ενισχυμένα}: \en{UNETR}, \en{SwinUNETR}.
  \item \textbf{\en{SSM/Mamba-based}}: \en{SegMamba}.
\end{itemize}

Κοινός στόχος είναι η παραγωγή μάσκας τμηματοποίησης με υψηλή ακρίβεια ορίων, ενώ οι διαφορές εντοπίζονται στον τρόπο
μοντελοποίησης συμφραζομένου και στον συμβιβασμό μεταξύ αποδοτικότητας και παγκόσμιας πληροφορίας.

\section{\en{UNet}}
\label{sec:unet}

Το \en{U\!-\!Net} \cite{ronneberger2015unet} αποτελεί σημείο αναφοράς στην ιατρική τμηματοποίηση. Η αρχιτεκτονική έχει
μορφή \textbf{\en{encoder--decoder}}:
\begin{itemize}
  \item Ο \textbf{κωδικοποιητής} (\en{encoder}) εφαρμόζει διαδοχικές συνελίξεις και υποδειγματοληψίες (\en{downsampling})
        για εξαγωγή ολοένα πιο αφηρημένων χαρακτηριστικών.
  \item Ο \textbf{αποκωδικοποιητής} (\en{decoder}) πραγματοποιεί υπερδειγματοληψία (\en{upsampling}) και ανακατασκευή της
        μάσκας στην αρχική ανάλυση.
  \item Οι \textbf{\en{skip connections}} μεταφέρουν υψηλής ανάλυσης χωρική πληροφορία από τον \en{encoder} προς τον \en{decoder},
        βοηθώντας την ακριβή αποκατάσταση ορίων.
\end{itemize}

Στη \en{3D} παραλλαγή, οι 2D συνελίξεις αντικαθίστανται από \en{3D} συνελίξεις, αυξάνοντας τη χωρική συνέπεια αλλά και το κόστος
υπολογισμού.

\section{\en{DynUNet} και αρχές αυτο-διαμόρφωσης}
\label{sec:dynunet}

Το \en{DynUNet} είναι δυναμική παραλλαγή \en{UNet} που προσαρμόζει την τοπολογία (βάθος, \en{strides}, μεγέθη πυρήνων)
με βάση την ανάλυση και το \en{patch size}. Η φιλοσοφία αυτή συνδέεται στενά με τις αρχές του \en{nnU-Net}, ενός
\textbf{\en{self-configuring}} πλαισίου που έδειξε ότι κατάλληλες επιλογές προεπεξεργασίας και αρχιτεκτονικής μπορούν να
δώσουν ισχυρά και σταθερά αποτελέσματα σε πλήθος προβλημάτων \cite{isensee2021nnunet}.

Πρακτικά, ο στόχος είναι να επιτευχθεί:
\begin{itemize}
  \item επαρκής κάλυψη πεδίου (\en{receptive field}) για παγκόσμιο πλαίσιο,
  \item χωρίς υπερβολική απώλεια ανάλυσης,
  \item και με δυνατότητα \textbf{\en{deep supervision}} σε ορισμένες υλοποιήσεις (βοηθώντας τη σύγκλιση).
\end{itemize}

\section{\en{SegResNet}}
\label{sec:segresnet}

Το \en{SegResNet} είναι \en{encoder--decoder} μοντέλο με \textbf{υπολειμματικά μπλοκ} (\en{ResNet blocks}) ως βασικό
δομικό στοιχείο. Μία επιδραστική περιγραφή αυτού του στυλ αρχιτεκτονικής στην \en{3D} \en{MRI} τμηματοποίηση παρουσιάζεται από
τον Myronenko \cite{myronenko2018segresnet}, όπου χρησιμοποιείται και πρόσθετη κλάδος \en{autoencoder} ως τακτικοποίηση
(\en{regularization}) σε περιβάλλον περιορισμένων δεδομένων.

Τα υπολειμματικά μπλοκ:
\begin{itemize}
  \item διευκολύνουν τη ροή κλίσεων σε βαθύτερα δίκτυα,
  \item επιτρέπουν πιο σταθερή εκπαίδευση,
  \item και συχνά βελτιώνουν τη γενίκευση όταν το \en{batch size} είναι μικρό (σύνηθες στην \en{3D} τμηματοποίηση).
\end{itemize}

\section{\en{AttentionUnet}}
\label{sec:attentionunet}

Η \en{Attention U-Net} εισάγει \textbf{\en{attention gates}} σε επιλεγμένα σημεία των \en{skip connections}, με σκοπό να
φιλτράρει πληροφορία που δεν είναι σχετική με το \en{ROI} και να ενισχύει χαρακτηριστικά που σχετίζονται με το όργανο
στόχο \cite{oktay2018attentionunet}. Η ιδέα είναι ιδιαίτερα χρήσιμη όταν:
\begin{itemize}
  \item το \en{ROI} είναι μικρό σε σχέση με το υπόβαθρο,
  \item υπάρχουν δομές με παρόμοια ένταση,
  \item ή η θέση/εμφάνιση του οργάνου μεταβάλλεται σημαντικά.
\end{itemize}

Στην τμηματοποίηση πλακούντα, όπου η μορφολογική μεταβλητότητα είναι μεγάλη και τα όρια συχνά ασαφή, η προσοχή μπορεί να
λειτουργήσει ως μηχανισμός \textbf{χωρικής εστίασης}.

\section{\en{UNETR}}
\label{sec:unetr}

Το \en{UNETR} \cite{hatamizadeh2022unetr} αποτελεί υβριδικό σχήμα, όπου ο \en{encoder} βασίζεται σε \en{Transformer}
(\en{ViT-like}) και ο \en{decoder} διατηρεί τη λογική ανακατασκευής τύπου \en{U\!-\!Net}. Η είσοδος χωρίζεται σε
\textbf{μη επικαλυπτόμενα \en{3D} \en{patches}}, τα οποία ενσωματώνονται σε ακολουθία και επεξεργάζονται με
\en{self-attention}.

Το πλεονέκτημα είναι η άμεση μοντελοποίηση παγκόσμιων συσχετίσεων, ενώ το μειονέκτημα είναι το υψηλότερο κόστος σε
μνήμη/χρόνο, ειδικά για μεγάλα \en{3D} \en{patch grids}. Για αυτό, η επιλογή ανάλυσης εισόδου και στρατηγικών \en{inference}
είναι κρίσιμη.

\section{\en{SwinUNETR}}
\label{sec:swinunetr}

Το \en{SwinUNETR} \cite{hatamizadeh2022swinunetr} αξιοποιεί \en{Swin Transformer} \cite{liu2021swin} ως
\textbf{ιεραρχικό} \en{encoder}. Αντί για πλήρη \en{attention} σε όλη την εικόνα, εφαρμόζει προσοχή εντός παραθύρων και
μετατοπίζει τα παράθυρα μεταξύ στρωμάτων, επιτρέποντας καλύτερη κλιμάκωση.

Στην πράξη, το \en{SwinUNETR} στοχεύει να συνδυάσει:
\begin{itemize}
  \item καλύτερη αποδοτικότητα από ``παραδοσιακούς'' \en{ViT-encoders},
  \item ισχυρό \en{global context} μέσω της ιεραρχικής μεταφοράς πληροφορίας,
  \item και ανάκτηση λεπτομέρειας με \en{decoder} τύπου \en{U\!-\!Net}.
\end{itemize}

\section{\en{SegMamba}}
\label{sec:segmamba}

Το \en{SegMamba} \cite{xing2024segmamba} είναι μοντέλο \en{3D} τμηματοποίησης που ενσωματώνει \en{SSM blocks} βασισμένα σε \en{Mamba}
\cite{gu2023mamba} σε \en{U-shape} δομή. Η βασική ιδέα είναι ότι το \en{Mamba} μπορεί να μοντελοποιήσει
μακρινές εξαρτήσεις με καλύτερη αποδοτικότητα σε μνήμη/χρόνο σε σχέση με τον κλασικό \en{self-attention}, κάτι που είναι
ελκυστικό σε \en{3D} όγκους.

Το \en{SegMamba} επιδιώκει:
\begin{itemize}
  \item μοντελοποίηση \textbf{μακράς εμβέλειας} εντός του όγκου (σε πολλαπλές κλίμακες),
  \item διατήρηση χωρικής πληροφορίας με συνελικτικά στοιχεία στον \en{decoder},
  \item ισορροπία μεταξύ παγκόσμιας συσχέτισης και πρακτικής αποδοτικότητας.
\end{itemize}

\section{Συζήτηση: Αναμενόμενοι συμβιβασμοί}
\label{sec:tradeoffs}

Συνοψίζοντας, οι οικογένειες μοντέλων διαφοροποιούνται ως προς το \textbf{πώς} συλλέγουν συμφραζόμενο:
\begin{itemize}
  \item Τα \en{CNN-based} μοντέλα τείνουν να είναι αποδοτικά και ισχυρά σε όρια/υφές, αλλά χρειάζονται κατάλληλο βάθος και
        σχεδίαση για επαρκές \en{global context}.
  \item Τα \en{Transformer-based} μοντέλα μοντελοποιούν πιο άμεσα παγκόσμιες συσχετίσεις, αλλά με αυξημένο κόστος και
        συχνά μεγαλύτερη ανάγκη σε δεδομένα/τακτικοποίηση.
  \item Τα \en{SSM/Mamba-based}μοντέλα στοχεύουν να προσφέρουν μακράς εμβέλειας μοντελοποίηση με καλύτερη κλιμάκωση, κάτι
        που είναι ιδιαίτερα ελκυστικό σε \en{3D} τμηματοποίηση.
\end{itemize}

Στα επόμενα κεφάλαια (πειραματικό μέρος) παρουσιάζεται το κοινό πρωτόκολλο εκπαίδευσης/αξιολόγησης που επιτρέπει να
αποτιμηθούν στην πράξη οι παραπάνω συμβιβασμοί για την τμηματοποίηση πλακούντα.
