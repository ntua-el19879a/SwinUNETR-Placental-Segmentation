%===============================
% chap3.tex (updated) — expanded 3.2, 3.3, 3.4
%===============================
\chapter{Μηχανική Μάθηση και Σύγχρονα Μοντέλα για Τμηματοποίηση}
\label{ch:ml_dl_models}

\InitialCharacter{Η} αυτόματη τμηματοποίηση με σύγχρονες προσεγγίσεις βασίζεται κυρίως σε επιβλεπόμενη Μηχανική Μάθηση,
όπου το μοντέλο εκπαιδεύεται να αναπαράγει ανθρώπινες επισημειώσεις. Η επιτυχία της Βαθιάς Μάθησης στην τμηματοποίηση
οφείλεται στην ικανότητα εκμάθησης πλούσιων ιεραρχικών αναπαραστάσεων από μεγάλα σύνολα δεδομένων, καθώς και στην
αποτελεσματική βελτιστοποίηση με \en{backpropagation}. Για γενικότερο πλαίσιο στη Βαθιά Μάθηση, παραπέμπουμε σε
κλασικές πηγές \cite{goodfellow2016deep,bishop2006pattern} και σε ανασκόπηση ειδικά για ιατρική απεικόνιση
\cite{litjens2017survey}.

\section{Επιβλεπόμενη μάθηση για τμηματοποίηση}
\label{sec:supervised_seg}

Έστω ότι διαθέτουμε σύνολο εκπαίδευσης $\mathcal{D}=\{(X_i, Y_i)\}_{i=1}^{N}$, όπου $X_i$ είναι \en{MRI} όγκος και $Y_i$
η αντίστοιχη δυαδική μάσκα πλακούντα. Θέλουμε να βρούμε παραμέτρους $\theta$ ώστε το μοντέλο $f_\theta$ να ελαχιστοποιεί
μία συνάρτηση κόστους:
\begin{equation}
\theta^\star = \arg\min_{\theta} \sum_{i=1}^{N} \mathcal{L}\big(f_\theta(X_i), Y_i\big).
\end{equation}

Στην τμηματοποίηση, η επιλογή $\mathcal{L}$ είναι καθοριστική. Η απλή \en{binary cross-entropy} είναι χρήσιμη, αλλά
συχνά συνδυάζεται με απώλειες επικάλυψης (π.χ. \en{Dice loss}) ώστε να αντιμετωπιστεί η ανισορροπία κλάσεων (το υπόβαθρο
καταλαμβάνει μεγάλο μέρος των \en{voxels}). Μία από τις γνωστές 3D αρχιτεκτονικές που ανέδειξαν τη χρησιμότητα τέτοιων
απωλειών είναι το \en{V-Net} \cite{milletari2016vnet}.

%------------------------------------------------
\section{Συνελικτικά Νευρωνικά Δίκτυα (\en{CNNs})}
\label{sec:cnns}

Τα \en{CNNs} αποτελούν την κυρίαρχη οικογένεια μοντέλων για εικόνες, καθώς ενσωματώνουν ισχυρές \textit{επαγωγικές
προκαταλήψεις} (\en{inductive biases}) όπως η τοπικότητα και η ισοδυναμία ως προς μετατοπίσεις. Οι συνελίξεις λειτουργούν
ως φίλτρα που ανιχνεύουν τοπικά πρότυπα (άκρα, υφές, σχήματα), ενώ η ιεραρχική στοίβαξη πολλών στρωμάτων επιτρέπει την
αναπαράσταση ολοένα και πιο σύνθετων δομών.

Ιστορικά, η επιτυχία των \en{CNNs} σε οπτικά δεδομένα εδράζεται σε κλασικές εργασίες \cite{lecun1998gradient} και στην
αναγέννηση της βαθιάς εκπαίδευσης σε μεγάλα δεδομένα \cite{krizhevsky2012imagenet}. Στη συνέχεια, αρχιτεκτονικές με
\textbf{υπολειμματικές συνδέσεις} (\en{residual connections}) επέτρεψαν την εκπαίδευση πολύ βαθιών δικτύων
\cite{he2016resnet}.

\subsection{Η συνέλιξη σε 2D/3D δεδομένα και η έννοια του \en{receptive field}}
Η βασική πράξη της συνέλιξης μπορεί να θεωρηθεί ως γραμμικός μετασχηματισμός με \textbf{κοινή χρήση βαρών} σε όλα τα χωρικά
σημεία. Για \en{3D} όγκους, μια 3D συνέλιξη εφαρμόζει πυρήνα $K$ διαστάσεων $k_x\times k_y\times k_z$ και παράγει
χαρακτηριστικά που κωδικοποιούν τοπικές χωρικές σχέσεις μέσα στον όγκο. Η τοπικότητα αυτή είναι ιδιαίτερα χρήσιμη στην
τμηματοποίηση, όπου τα όρια ενός οργάνου καθορίζονται από \textit{τοπικές} μεταβολές έντασης και υφής.

Καθώς το δίκτυο βαθαίνει, το \textbf{\en{receptive field}} ενός νευρώνα μεγαλώνει (μέσω διαδοχικών συνελίξεων και
υποδειγματοληψιών), επιτρέποντας στο μοντέλο να ενσωματώνει μεγαλύτερο συμφραζόμενο. Παρ’ όλα αυτά, η αύξηση αυτή είναι
\textit{έμμεση} και εξαρτάται από το βάθος/σχεδίαση του δικτύου.

\subsection{Αρχιτεκτονικές πυκνής πρόβλεψης: \en{FCN}, \en{encoder--decoder} και \en{U-shaped} σχεδίαση}
Η τμηματοποίηση απαιτεί \textbf{πυκνή} πρόβλεψη (ετικέτα ανά \en{pixel/voxel}). Μια θεμελιώδης ιδέα για αυτόν τον σκοπό
είναι τα \en{Fully Convolutional Networks (FCNs)}, τα οποία αντικαθιστούν τα πλήρως συνδεδεμένα στρώματα με συνελικτικά,
ώστε το δίκτυο να παράγει χωρικούς χάρτες εξόδου \cite{long2015fcn}.

Στην ιατρική τμηματοποίηση, η οικογένεια \en{U-Net} είναι ιδιαίτερα επιδραστική, καθώς συνδυάζει:
\begin{itemize}
  \item έναν \textbf{κωδικοποιητή} (\en{encoder}) που συμπιέζει την πληροφορία σε πολλαπλές κλίμακες (πιάνοντας «τι»),
  \item έναν \textbf{αποκωδικοποιητή} (\en{decoder}) που ανακατασκευάζει την ανάλυση (πιάνοντας «πού»),
  \item \textbf{συνδέσεις παράκαμψης} (\en{skip connections}) που μεταφέρουν λεπτομέρεια υψηλής ανάλυσης από τον κωδικοποιητή
        στον αποκωδικοποιητή, βελτιώνοντας την ακρίβεια ορίων \cite{ronneberger2015unet}.
\end{itemize}
Για ογκομετρικά δεδομένα, η λογική αυτή επεκτάθηκε σε \en{3D U-Net} με 3D συνελίξεις, επιτρέποντας αξιοποίηση της
πληροφορίας κατά μήκος όλων των αξόνων του όγκου \cite{cicek20163dunet}.

\subsection{Κανονικοποίηση και σταθεροποίηση εκπαίδευσης}
Σε βαθιά δίκτυα, η σταθερότητα εκπαίδευσης επηρεάζεται από την κατανομή ενεργοποιήσεων. Η \en{Batch Normalization} είναι
κλασική τεχνική επιτάχυνσης και σταθεροποίησης \cite{ioffe2015batchnorm}, όμως στη 3D τμηματοποίηση το \en{batch size}
είναι συχνά πολύ μικρό λόγω μνήμης. Σε τέτοιες περιπτώσεις, τεχνικές όπως η \en{Group Normalization} μπορεί να είναι
πρακτικά πιο σταθερές \cite{wu2018groupnorm}.

\subsection{Προσοχή μέσα σε \en{CNNs}: \en{Attention U-Net}}
Η «προσοχή» δεν ταυτίζεται απαραίτητα με τους \en{Transformers}. Στα \en{CNN}-βασισμένα μοντέλα, έχουν προταθεί
\textbf{\en{attention gates}} που «φιλτράρουν» τις \en{skip connections} ώστε να τονίζουν περιοχές σχετικές με το όργανο
στόχο. Ένα κλασικό παράδειγμα είναι το \en{Attention U-Net} \cite{oktay2018attentionunet}, το οποίο μπορεί να βελτιώσει
την εστίαση σε μικρές ή δύσκολες δομές χωρίς να απαιτεί πλήρη \en{self-attention} σε όλο τον όγκο.

\subsection{Πλεονεκτήματα και περιορισμοί των \en{CNNs}}
Τα κύρια πλεονεκτήματα για τμηματοποίηση είναι:
\begin{itemize}
  \item Υψηλή \textbf{αποδοτικότητα} (η συνέλιξη είναι υπολογιστικά ευνοϊκή).
  \item Έμφαση σε \textbf{τοπικά μορφολογικά στοιχεία}, χρήσιμα για ακριβή όρια.
  \item Καλύτερη συμπεριφορά σε \textbf{μικρότερα σύνολα δεδομένων} σε σχέση με καθαρά \en{Transformer} μοντέλα.
\end{itemize}

Κύριος περιορισμός είναι ότι η \textbf{παγκόσμια πληροφορία} (\en{global context}) δεν είναι άμεσα διαθέσιμη: οι μακρινές
χωρικές συσχετίσεις μοντελοποιούνται έμμεσα μέσω βάθους/υποδειγματοληψιών, και η αποτελεσματικότητα αυτής της διαδικασίας
εξαρτάται έντονα από τη σχεδίαση του δικτύου (βάθος, \en{dilation}, κ.\,λπ.).

%------------------------------------------------
\section{Μοντέλα \en{Attention} και \en{Transformers}}
\label{sec:transformers}

Οι \en{Transformers} εισήχθησαν ως αρχιτεκτονική ακολουθιών με μηχανισμό \textbf{αυτο-προσοχής} (\en{self-attention})
\cite{vaswani2017attention}. Η ιδέα είναι ότι κάθε στοιχείο της εισόδου μπορεί να «συσχετιστεί» άμεσα με άλλα στοιχεία,
επιτρέποντας μοντελοποίηση μακρινών εξαρτήσεων χωρίς να απαιτείται βαθιά στοίβαξη συνελίξεων.

\subsection{Βασικός μηχανισμός \en{self-attention}}
Για μια ακολουθία $n$ \en{tokens} (π.\,χ. \en{patches} εικόνας), ο \en{self-attention} υπολογίζει:
\begin{equation}
\mathrm{Att}(Q,K,V)=\mathrm{softmax}\!\left(\frac{QK^\top}{\sqrt{d_k}}\right)V,
\end{equation}
όπου $Q,K,V$ (\en{queries, keys, values}) είναι γραμμικοί μετασχηματισμοί των \en{tokens}.
Η \en{multi-head} εκδοχή εφαρμόζει πολλαπλές τέτοιες προσοχές παράλληλα, ώστε το μοντέλο να μαθαίνει διαφορετικούς τύπους
συσχετίσεων.

\subsection{Γιατί ο \en{attention} είναι ακριβός σε όραση και ιδιαίτερα σε \en{3D}}
Ο κλασικός \en{self-attention} έχει υπολογιστικό και μνημονικό κόστος $O(n^2)$ ως προς το πλήθος \en{tokens} $n$.
Σε \en{NLP}, το $n$ είναι το μήκος πρότασης. Στην όραση, όμως, το $n$ προκύπτει από την αποσύνθεση της εικόνας/όγκου σε
\en{patches}. Για \en{2D} εικόνα, $n \approx (H\cdot W)/P^2$, ενώ για \en{3D} όγκο $n \approx (H\cdot W\cdot D)/P^3$.
Ακόμη και με σχετικά μεγάλα \en{patches}, το $n$ μπορεί να είναι πολύ μεγάλο, κάνοντας τον πλήρη \en{attention} δύσχρηστο
σε \en{GPU} μνήμη και χρόνο.

Ο \en{Vision Transformer (ViT)} \cite{dosovitskiy2021vit} αντιμετώπισε το πρόβλημα εισάγοντας \en{patch embedding} και
μαθαίνοντας σε μεγάλες κλίμακες δεδομένων. Ωστόσο, για ιατρικά δεδομένα (συχνά μικρότερα σύνολα, 3D όγκοι), απαιτούνται
πρόσθετες σχεδιαστικές επιλογές ώστε να είναι πρακτική η εκπαίδευση.

\subsection{Στρατηγικές μείωσης κόστους: ιεραρχία, τοπικά παράθυρα και προσεγγίσεις γραμμικής προσοχής}
Μερικές από τις κύριες κατευθύνσεις για να καταστεί ο \en{attention} πρακτικός σε μεγάλες εισόδους είναι:
\begin{itemize}
  \item \textbf{Ιεραρχική αναπαράσταση} με υποδειγματοληψίες, ώστε να μειώνεται σταδιακά το πλήθος \en{tokens}.
  \item \textbf{Τοπική προσοχή σε παράθυρα} (\en{window attention}): ο \en{Swin Transformer} υπολογίζει προσοχή σε τοπικά
        παράθυρα και εισάγει \en{shifted windows} για ανταλλαγή πληροφορίας μεταξύ παραθύρων, επιτυγχάνοντας καλύτερη
        κλιμάκωση \cite{liu2021swin}.
  \item \textbf{Προσεγγίσεις υπο-τετραγωνικής προσοχής}: π.\,χ. χαμηλόβαθμη προβολή (\en{Linformer}) \cite{wang2020linformer}
        ή στοχαστικές προσεγγίσεις με \en{random features} (\en{Performer}) \cite{choromanski2021performer}.
\end{itemize}
Στο πλαίσιο ιατρικής τμηματοποίησης, συχνά προτιμώνται οι δύο πρώτες στρατηγικές (ιεραρχία/παράθυρα), επειδή διατηρούν
καλύτερα τη χωρική δομή και είναι πιο «φιλικές» σε \en{3D} δεδομένα.

\subsection{Υβριδικά \en{Transformer}--\en{U-shape} μοντέλα για 3D τμηματοποίηση}
Υβριδικά μοντέλα όπως τα \en{UNETR} \cite{hatamizadeh2022unetr} και \en{SwinUNETR} \cite{hatamizadeh2022swinunetr}
χρησιμοποιούν \en{Transformer}-ενισχυμένους κωδικοποιητές για καλύτερη κατανόηση συμφραζομένου, διατηρώντας ταυτόχρονα
\en{decoder} που ανακτά λεπτομέρεια υψηλής ανάλυσης, όπως στο \en{U-Net}. Έτσι επιχειρείται συνδυασμός:
\begin{itemize}
  \item \textbf{παγκόσμιας/μακρινής συσχέτισης} (ισχυρό σημείο των \en{Transformers})
  \item με \textbf{ακριβή χωρική ανακατασκευή} (ισχυρό σημείο της \en{U-shaped} αποκωδικοποίησης).
\end{itemize}

%------------------------------------------------
\section{Μοντέλα \en{State Space} και \en{Mamba}}
\label{sec:ssm_mamba}

Τα \en{State Space Models} (\en{SSMs}) αποτελούν κλάση μοντέλων ακολουθιών που περιγράφουν την εξέλιξη μιας κρυφής
κατάστασης μέσα στον χρόνο/δείκτη ακολουθίας. Η βασική διακριτή μορφή μπορεί να γραφεί ως:
\begin{align}
h_{t} &= A h_{t-1} + B x_{t}, \\
y_{t} &= C h_{t} + D x_{t},
\end{align}
όπου $x_t$ το στοιχείο εισόδου, $h_t$ η κρυφή κατάσταση και $y_t$ η έξοδος. Παρότι η παραπάνω μορφή είναι κλασική,
σύγχρονες προσεγγίσεις αναπτύσσουν \textbf{δομημένα} \en{SSMs} που (α) κλιμακώνονται σε μεγάλα μήκη ακολουθίας και
(β) είναι ανταγωνιστικά σε ποιότητα αναπαράστασης.

\subsection{Δομημένα \en{SSMs} και μακρές εξαρτήσεις}
Μία επιδραστική γραμμή εργασιών είναι τα \textbf{Structured State Space Models}, όπου η δομή των πινάκων/πυρήνων
επιτρέπει αποδοτικό υπολογισμό και καλή συμπεριφορά σε πολύ μεγάλες ακολουθίες. Το \en{S4} είναι χαρακτηριστικό παράδειγμα
αυτής της κατηγορίας και έχει μελετηθεί ως εναλλακτική στους \en{Transformers} για μακρές εξαρτήσεις
\cite{gu2021s4}.

\subsection{\en{Mamba}: \en{Selective State Spaces} και γραμμική κλιμάκωση}
Το \en{Mamba} εισάγει μία πρακτική και αποδοτική υλοποίηση \en{SSM} με \textbf{selective state spaces}
\cite{gu2023mamba}. Η βασική ιδέα είναι ότι το μοντέλο μπορεί να διατηρεί ικανότητα «περιεχομενο-εξαρτώμενης» επιλογής
(\textit{content-based}) που θυμίζει την προσαρμοστικότητα του \en{attention}, αλλά με κόστος πιο κοντά στο \textbf{γραμμικό}
ως προς το μήκος ακολουθίας. Αυτό το χαρακτηριστικό είναι ιδιαίτερα σημαντικό όταν τα \en{tokens} είναι πολλά (όπως σε
\en{3D} όγκους), όπου ο τετραγωνικός \en{attention} γίνεται απαγορευτικός.

\subsection{Από την ακολουθία στην εικόνα: πώς χρησιμοποιούνται \en{SSMs} σε \en{3D} τμηματοποίηση}
Για να εφαρμοστούν \en{SSMs} σε εικόνες/όγκους, απαιτείται ένας τρόπος «σειριοποίησης» ή/και ανταλλαγής πληροφορίας
μεταξύ αξόνων/περιοχών. Πρακτικά, αυτό υλοποιείται με:
\begin{itemize}
  \item αναπαράσταση του όγκου σε \en{tokens} (π.\,χ. \en{patch tokens} σε μία ή περισσότερες κλίμακες),
  \item εφαρμογή \en{SSM}-blocks σε κατάλληλες διατάξεις που επιτρέπουν μακρινή συσχέτιση,
  \item συνδυασμό με \en{U-shaped} ροή πολλαπλών κλιμάκων ώστε να διατηρηθεί λεπτομέρεια.
\end{itemize}
Ένα πρόσφατο παράδειγμα που ενσωματώνει \en{Mamba}-blocks σε 3D τμηματοποίηση είναι το \en{SegMamba}
\cite{xing2024segmamba}, το οποίο στοχεύει να αποτυπώσει \textbf{long-range} συσχετίσεις σε ογκομετρικά χαρακτηριστικά με
καλύτερη αποδοτικότητα σε σχέση με πλήρη \en{attention}.

\subsection{Συγκριτική οπτική: \en{CNNs} έναντι \en{Transformers} έναντι \en{SSMs}}
Συνοψίζοντας σε επίπεδο θεωρίας:
\begin{itemize}
  \item \textbf{\en{CNNs}}: ισχυρή τοπική μοντελοποίηση και αποδοτικότητα, αλλά έμμεση/περιορισμένη παγκόσμια συσχέτιση.
  \item \textbf{\en{Transformers}}: άμεση παγκόσμια συσχέτιση, αλλά υψηλό κόστος $O(n^2)$ που επιβαρύνει ιδιαίτερα \en{3D}.
  \item \textbf{\en{SSMs}/\en{Mamba}}: στόχος η μακράς εμβέλειας μοντελοποίηση με καλύτερη κλιμάκωση, αν και η επιτυχία τους
        εξαρτάται από τον τρόπο ενσωμάτωσης σε χωρικά δεδομένα και τη σχεδίαση πολλαπλών κλιμάκων.
\end{itemize}

\section{Βελτιστοποίηση και πρακτικές εκπαίδευσης}
\label{sec:training_practices}

Η εκπαίδευση βαθιών δικτύων πραγματοποιείται συνήθως με στοχαστική κατάβαση κλίσης και παραλλαγές της.
Ο \en{AdamW} είναι διαδεδομένος βελτιστοποιητής που αποσυνδέει το \en{weight decay} από την ενημέρωση παραμέτρων
\cite{loshchilov2019adamw}. Επιπλέον, σε 3D τμηματοποίηση είναι συνήθη:
\begin{itemize}
  \item \textbf{Μικρό \en{batch size}} λόγω μνήμης.
  \item \textbf{Εκπαίδευση με \en{patches}} και ισορροπία θετικών/αρνητικών δειγμάτων.
  \item \textbf{Προγράμματα μάθησης} (\en{learning rate schedules}) για καλύτερη σύγκλιση.
  \item \textbf{Κανονικοποίηση} (π.χ. \en{GroupNorm}) όταν το \en{batch size} είναι πολύ μικρό.
\end{itemize}

Στην παρούσα εργασία, οι παραπάνω επιλογές χρησιμοποιούνται με τρόπο που επιτρέπει \textbf{δίκαιη σύγκριση} μεταξύ
αρχιτεκτονικών, ώστε να απομονωθεί η επίδραση της ίδιας της σχεδίασης του μοντέλου.
